nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_loc_glo_no_que
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 4.460555076599121
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 1.4352102279663086
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.2730655670166016
INFO:root:Accuracy qa: 59.58844133099825
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 1.1086958646774292
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.139648675918579
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.2961280345916748
INFO:root:Accuracy qa: 61.25218914185639
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 1.0626178979873657
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 1.1409789323806763
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 1.0557730197906494
INFO:root:Accuracy qa: 65.32399299474606
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 0.9835891127586365
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 0.9271191358566284
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 0.8723421096801758
INFO:root:Accuracy qa: 67.49124343257444
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.8807450532913208
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.8344409465789795
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 0.8079088926315308
INFO:root:Accuracy qa: 69.57092819614711
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 0.9651181697845459
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.767186164855957
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.8029948472976685
INFO:root:Accuracy qa: 70.24956217162872
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.837215781211853
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.7406390309333801
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.6265472173690796
INFO:root:Accuracy qa: 69.13309982486865
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.6333156824111938
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.6319911479949951
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.5973448157310486
INFO:root:Accuracy qa: 70.86252189141857
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.653740644454956
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.4306797683238983
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.5490566492080688
INFO:root:Accuracy qa: 73.42381786339755
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.44628310203552246
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.3695094585418701
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.42513319849967957
INFO:root:Accuracy qa: 72.83274956217163
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.4393978714942932
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.3590816557407379
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.35933464765548706
INFO:root:Accuracy qa: 72.98598949211909
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.29611387848854065
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.3666456341743469
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.36843815445899963
INFO:root:Accuracy qa: 72.28546409807356
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.26467037200927734
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.2877001166343689
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.342399001121521
INFO:root:Accuracy qa: 72.67950963222417
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.2815229296684265
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.2775084376335144
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.2469577044248581
INFO:root:Accuracy qa: 72.83274956217163
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.24718549847602844
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.25464093685150146
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.23275265097618103
INFO:root:Accuracy qa: 72.8984238178634
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 9
INFO:root:the best train acc ------ 73.42381786339755
INFO:root:********************************************************
Audio Counting num: 847
Audio Cmp num: 390
Visual Counting num: 933
Visual Loc num: 937
AV Ext num: 807
AV counting num: 910
AV Loc num: 582
AV Cmp num: 681
AV Temporal num: 533
Audio Counting num: 847
Audio Cmp num: 390
Visual Counting num: 933
Visual Loc num: 937
AV Ext num: 807
AV counting num: 910
AV Loc num: 582
AV Cmp num: 681
AV Temporal num: 533
Audio Counting num: 847
Audio Cmp num: 390
Visual Counting num: 933
Visual Loc num: 937
AV Ext num: 807
AV counting num: 910
AV Loc num: 582
AV Cmp num: 681
AV Temporal num: 533
INFO:root:Audio Counting Accuracy: 83.28 %
INFO:root:Audio Cmp Accuracy: 65.66 %
INFO:root:Audio Accuracy: 76.78 %
INFO:root:Visual Counting Accuracy: 77.94 %
INFO:root:Visual Loc Accuracy: 76.49 %
INFO:root:Visual Accuracy: 77.21 %
INFO:root:AV Ext Accuracy: 81.68 %
INFO:root:AV counting Accuracy: 71.94 %
INFO:root:AV Loc Accuracy: 63.26 %
INFO:root:AV Cmp Accuracy: 61.85 %
INFO:root:AV Temporal Accuracy: 64.84 %
INFO:root:AV Accuracy: 68.94 %
INFO:root:Overall Accuracy: 72.52 %
Audio Counting num: 847
Audio Cmp num: 390
Visual Counting num: 933
Visual Loc num: 937
AV Ext num: 807
AV counting num: 910
AV Loc num: 582
AV Cmp num: 681
AV Temporal num: 533
