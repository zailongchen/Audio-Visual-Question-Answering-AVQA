nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_0.4glo
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 5.576214790344238
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 2.009133815765381
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.828673005104065
INFO:root:Accuracy qa: 61.843257443082315
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 1.5747911930084229
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.6903581619262695
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.5856608152389526
INFO:root:Accuracy qa: 65.1707530647986
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 1.3900949954986572
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 1.1587176322937012
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 1.1031785011291504
INFO:root:Accuracy qa: 66.63747810858143
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 0.9704335927963257
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 0.9651424884796143
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 0.8141599893569946
INFO:root:Accuracy qa: 68.67338003502627
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.777702808380127
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.6923350691795349
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 0.717786431312561
INFO:root:Accuracy qa: 70.1401050788091
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 0.8625708818435669
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.7429196834564209
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.7114842534065247
INFO:root:Accuracy qa: 70.53415061295972
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.7426520586013794
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.6509296298027039
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.5518112182617188
INFO:root:Accuracy qa: 68.58581436077058
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.5548271536827087
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.48244237899780273
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.46982482075691223
INFO:root:Accuracy qa: 71.58493870402802
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.511053204536438
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.3547946810722351
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.40897125005722046
INFO:root:Accuracy qa: 74.67162872154115
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.27412426471710205
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.2741740345954895
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.23864543437957764
INFO:root:Accuracy qa: 74.6278458844133
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.29663223028182983
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.2263719141483307
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.22296752035617828
INFO:root:Accuracy qa: 74.5183887915937
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.16986839473247528
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.2223202884197235
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.18049106001853943
INFO:root:Accuracy qa: 73.861646234676
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.18948355317115784
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.15205682814121246
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.2057115137577057
INFO:root:Accuracy qa: 74.36514886164623
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.19269149005413055
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.167677104473114
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.16548117995262146
INFO:root:Accuracy qa: 74.16812609457092
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.13749073445796967
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.1380206048488617
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.11445474624633789
INFO:root:Accuracy qa: 74.43082311733801
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 9
INFO:root:the best train acc ------ 74.67162872154115
INFO:root:********************************************************
Audio Counting num: 845
Audio Cmp num: 404
Visual Counting num: 948
Visual Loc num: 956
AV Ext num: 820
AV counting num: 936
AV Loc num: 570
AV Cmp num: 710
AV Temporal num: 539
Audio Counting num: 845
Audio Cmp num: 404
Visual Counting num: 948
Visual Loc num: 956
AV Ext num: 820
AV counting num: 936
AV Loc num: 570
AV Cmp num: 710
AV Temporal num: 539
Audio Counting num: 845
Audio Cmp num: 404
Visual Counting num: 948
Visual Loc num: 956
AV Ext num: 820
AV counting num: 936
AV Loc num: 570
AV Cmp num: 710
AV Temporal num: 539
Audio Counting num: 845
Audio Cmp num: 404
Visual Counting num: 948
Visual Loc num: 956
AV Ext num: 820
AV counting num: 936
AV Loc num: 570
AV Cmp num: 710
AV Temporal num: 539
INFO:root:Audio Counting Accuracy: 83.09 %
INFO:root:Audio Cmp Accuracy: 68.01 %
INFO:root:Audio Accuracy: 77.53 %
INFO:root:Visual Counting Accuracy: 79.20 %
INFO:root:Visual Loc Accuracy: 78.04 %
INFO:root:Visual Accuracy: 78.61 %
INFO:root:AV Ext Accuracy: 83.00 %
INFO:root:AV counting Accuracy: 73.99 %
INFO:root:AV Loc Accuracy: 61.96 %
INFO:root:AV Cmp Accuracy: 64.49 %
INFO:root:AV Temporal Accuracy: 65.57 %
INFO:root:AV Accuracy: 70.15 %
INFO:root:Overall Accuracy: 73.70 %
