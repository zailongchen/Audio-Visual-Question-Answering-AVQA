nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_0.7loc
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 6.6194658279418945
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 1.8173636198043823
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.5611786842346191
INFO:root:Accuracy qa: 61.42732049036778
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 1.344626784324646
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.534475326538086
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.3636457920074463
INFO:root:Accuracy qa: 65.21453590192644
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 1.2373839616775513
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 1.2140675783157349
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 1.2975585460662842
INFO:root:Accuracy qa: 67.77583187390543
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 1.1233813762664795
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 1.0799469947814941
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 1.096382975578308
INFO:root:Accuracy qa: 69.41768826619965
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.9717137813568115
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.9207280278205872
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 1.007599949836731
INFO:root:Accuracy qa: 69.83362521891418
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 1.046743631362915
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.8953049182891846
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.8284314870834351
INFO:root:Accuracy qa: 70.16199649737302
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.8427323698997498
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.7487760782241821
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.7968335151672363
INFO:root:Accuracy qa: 69.59281961471103
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.6452856659889221
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.6701434850692749
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.716815710067749
INFO:root:Accuracy qa: 72.28546409807356
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.6783754229545593
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.4137805104255676
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.4718405604362488
INFO:root:Accuracy qa: 74.6059544658494
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.39854592084884644
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.3856705129146576
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.3549167513847351
INFO:root:Accuracy qa: 74.49649737302977
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.38632118701934814
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.2847045063972473
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.2773716449737549
INFO:root:Accuracy qa: 74.38704028021016
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.212406188249588
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.2759149968624115
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.270601749420166
INFO:root:Accuracy qa: 73.77408056042032
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.21641021966934204
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.22307877242565155
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.3008430600166321
INFO:root:Accuracy qa: 73.48949211908932
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.21368731558322906
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.2155473232269287
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.17280727624893188
INFO:root:Accuracy qa: 73.9492119089317
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.18918675184249878
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.18534639477729797
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.20375032722949982
INFO:root:Accuracy qa: 73.97110332749563
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 11
INFO:root:the best train acc ------ 74.10245183887916
INFO:root:********************************************************
Audio Counting num: 846
Audio Cmp num: 409
Visual Counting num: 948
Visual Loc num: 964
AV Ext num: 806
AV counting num: 936
AV Loc num: 580
AV Cmp num: 730
AV Temporal num: 536
INFO:root:Audio Counting Accuracy: 83.19 %
INFO:root:Audio Cmp Accuracy: 68.86 %
INFO:root:Audio Accuracy: 77.90 %
INFO:root:Visual Counting Accuracy: 79.20 %
INFO:root:Visual Loc Accuracy: 78.69 %
INFO:root:Visual Accuracy: 78.94 %
INFO:root:AV Ext Accuracy: 81.58 %
INFO:root:AV counting Accuracy: 73.99 %
INFO:root:AV Loc Accuracy: 63.04 %
INFO:root:AV Cmp Accuracy: 66.30 %
INFO:root:AV Temporal Accuracy: 65.21 %
INFO:root:AV Accuracy: 70.41 %
INFO:root:Overall Accuracy: 73.99 %
Audio Counting num: 846
Audio Cmp num: 409
Visual Counting num: 948
Visual Loc num: 964
AV Ext num: 806
AV counting num: 936
AV Loc num: 580
AV Cmp num: 730
AV Temporal num: 536
Audio Counting num: 846
Audio Cmp num: 409
Visual Counting num: 948
Visual Loc num: 964
AV Ext num: 806
AV counting num: 936
AV Loc num: 580
AV Cmp num: 730
AV Temporal num: 536
Audio Counting num: 846
Audio Cmp num: 409
Visual Counting num: 948
Visual Loc num: 964
AV Ext num: 806
AV counting num: 936
AV Loc num: 580
AV Cmp num: 730
AV Temporal num: 536
