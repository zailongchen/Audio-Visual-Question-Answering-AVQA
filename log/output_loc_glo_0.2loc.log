nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_0.2loc
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 4.850512981414795
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 1.4525823593139648
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.277331829071045
INFO:root:Accuracy qa: 61.62434325744308
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 1.134182333946228
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.2405993938446045
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.2097446918487549
INFO:root:Accuracy qa: 64.88616462346761
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 1.0199313163757324
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 1.0537246465682983
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 1.0837851762771606
INFO:root:Accuracy qa: 67.18476357267951
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 0.9369326829910278
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 0.989901602268219
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 0.8126834034919739
INFO:root:Accuracy qa: 69.4614711033275
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.7543601393699646
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.7591813802719116
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 0.7808270454406738
INFO:root:Accuracy qa: 70.31523642732049
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 0.975456714630127
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.7573490142822266
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.7491527795791626
INFO:root:Accuracy qa: 71.25656742556917
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.797369122505188
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.7583068609237671
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.6430125832557678
INFO:root:Accuracy qa: 70.29334500875656
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.5644727349281311
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.5627719759941101
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.6215075254440308
INFO:root:Accuracy qa: 72.00087565674255
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.5330396294593811
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.3816896677017212
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.4288449287414551
INFO:root:Accuracy qa: 74.40893169877408
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.3630395531654358
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.3313349485397339
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.31611326336860657
INFO:root:Accuracy qa: 74.75919439579685
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.3402138948440552
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.2611507475376129
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.2704131603240967
INFO:root:Accuracy qa: 74.45271453590193
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.21754705905914307
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.285006046295166
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.23938176035881042
INFO:root:Accuracy qa: 73.99299474605955
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.22555550932884216
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.229253888130188
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.286945104598999
INFO:root:Accuracy qa: 74.2338003502627
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.23240038752555847
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.23010456562042236
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.19435270130634308
INFO:root:Accuracy qa: 73.75218914185639
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.21204730868339539
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.2001485526561737
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.220392644405365
INFO:root:Accuracy qa: 74.14623467600701
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 10
INFO:root:the best train acc ------ 74.75919439579685
INFO:root:********************************************************
Audio Counting num: 848
Audio Cmp num: 410
Visual Counting num: 937
Visual Loc num: 952
AV Ext num: 800
AV counting num: 928
AV Loc num: 582
AV Cmp num: 737
AV Temporal num: 536
INFO:root:Audio Counting Accuracy: 83.38 %
INFO:root:Audio Cmp Accuracy: 69.02 %
INFO:root:Audio Accuracy: 78.09 %
INFO:root:Visual Counting Accuracy: 78.28 %
INFO:root:Visual Loc Accuracy: 77.71 %
INFO:root:Visual Accuracy: 77.99 %
INFO:root:AV Ext Accuracy: 80.97 %
INFO:root:AV counting Accuracy: 73.36 %
INFO:root:AV Loc Accuracy: 63.26 %
INFO:root:AV Cmp Accuracy: 66.94 %
INFO:root:AV Temporal Accuracy: 65.21 %
INFO:root:AV Accuracy: 70.31 %
INFO:root:Overall Accuracy: 73.72 %
Audio Counting num: 848
Audio Cmp num: 410
Visual Counting num: 937
Visual Loc num: 952
AV Ext num: 800
AV counting num: 928
AV Loc num: 582
AV Cmp num: 737
AV Temporal num: 536
Audio Counting num: 848
Audio Cmp num: 410
Visual Counting num: 937
Visual Loc num: 952
AV Ext num: 800
AV counting num: 928
AV Loc num: 582
AV Cmp num: 737
AV Temporal num: 536
Audio Counting num: 848
Audio Cmp num: 410
Visual Counting num: 937
Visual Loc num: 952
AV Ext num: 800
AV counting num: 928
AV Loc num: 582
AV Cmp num: 737
AV Temporal num: 536
