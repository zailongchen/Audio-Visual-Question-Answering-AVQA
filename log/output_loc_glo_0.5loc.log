nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_0.5loc
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 5.907984256744385
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 1.6536593437194824
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.4715806245803833
INFO:root:Accuracy qa: 61.274080560420316
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 1.2506506443023682
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.4305343627929688
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.3223857879638672
INFO:root:Accuracy qa: 65.10507880910683
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 1.1827360391616821
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 1.1924421787261963
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 1.2273480892181396
INFO:root:Accuracy qa: 67.38178633975481
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 1.0316234827041626
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 1.0465857982635498
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 0.9279921650886536
INFO:root:Accuracy qa: 69.72416812609457
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.9267510771751404
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.8437016606330872
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 0.8791171908378601
INFO:root:Accuracy qa: 71.10332749562171
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 1.0781123638153076
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.8003153800964355
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.8458411693572998
INFO:root:Accuracy qa: 71.43169877408056
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.8824037313461304
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.8178818225860596
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.6856164932250977
INFO:root:Accuracy qa: 70.24956217162872
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.6246383190155029
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.5705398917198181
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.657802402973175
INFO:root:Accuracy qa: 72.02276707530648
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.6079012155532837
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.44523173570632935
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.5302340388298035
INFO:root:Accuracy qa: 74.19001751313485
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.4088095426559448
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.40337270498275757
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.38222867250442505
INFO:root:Accuracy qa: 73.81786339754817
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.41076362133026123
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.2950045168399811
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.3398320972919464
INFO:root:Accuracy qa: 74.10245183887916
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.2317151129245758
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.30101725459098816
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.3186352252960205
INFO:root:Accuracy qa: 73.77408056042032
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.2566636800765991
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.29674071073532104
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.33601200580596924
INFO:root:Accuracy qa: 73.577057793345
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.267241895198822
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.27122798562049866
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.22478362917900085
INFO:root:Accuracy qa: 73.79597197898424
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.23047029972076416
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.2394810914993286
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.22765345871448517
INFO:root:Accuracy qa: 73.88353765323993
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 9
INFO:root:the best train acc ------ 74.19001751313485
INFO:root:********************************************************
Audio Counting num: 847
Audio Cmp num: 410
Visual Counting num: 935
Visual Loc num: 968
AV Ext num: 812
AV counting num: 927
AV Loc num: 591
AV Cmp num: 720
AV Temporal num: 541
INFO:root:Audio Counting Accuracy: 83.28 %
INFO:root:Audio Cmp Accuracy: 69.02 %
INFO:root:Audio Accuracy: 78.03 %
INFO:root:Visual Counting Accuracy: 78.11 %
INFO:root:Visual Loc Accuracy: 79.02 %
INFO:root:Visual Accuracy: 78.57 %
INFO:root:AV Ext Accuracy: 82.19 %
INFO:root:AV counting Accuracy: 73.28 %
INFO:root:AV Loc Accuracy: 64.24 %
INFO:root:AV Cmp Accuracy: 65.40 %
INFO:root:AV Temporal Accuracy: 65.82 %
INFO:root:AV Accuracy: 70.47 %
INFO:root:Overall Accuracy: 73.95 %
Audio Counting num: 847
Audio Cmp num: 410
Visual Counting num: 935
Visual Loc num: 968
AV Ext num: 812
AV counting num: 927
AV Loc num: 591
AV Cmp num: 720
AV Temporal num: 541
Audio Counting num: 847
Audio Cmp num: 410
Visual Counting num: 935
Visual Loc num: 968
AV Ext num: 812
AV counting num: 927
AV Loc num: 591
AV Cmp num: 720
AV Temporal num: 541
Audio Counting num: 847
Audio Cmp num: 410
Visual Counting num: 935
Visual Loc num: 968
AV Ext num: 812
AV counting num: 927
AV Loc num: 591
AV Cmp num: 720
AV Temporal num: 541
