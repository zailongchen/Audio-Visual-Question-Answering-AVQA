nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_0.01glo
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 4.177272796630859
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 1.2056100368499756
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.050039529800415
INFO:root:Accuracy qa: 61.1646234676007
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 0.9137415885925293
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.0239866971969604
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.0213476419448853
INFO:root:Accuracy qa: 64.33887915936953
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 0.9129106402397156
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 0.9362887144088745
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 0.846169650554657
INFO:root:Accuracy qa: 66.5061295971979
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 0.8169289827346802
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 0.7584760189056396
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 0.7473388910293579
INFO:root:Accuracy qa: 69.13309982486865
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.6859343647956848
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.6032384634017944
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 0.6812313199043274
INFO:root:Accuracy qa: 69.52714535901926
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 0.809165358543396
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.6084128618240356
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.6209676265716553
INFO:root:Accuracy qa: 71.16900175131349
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.7408227324485779
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.6232905983924866
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.502259373664856
INFO:root:Accuracy qa: 69.96497373029773
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.5204402804374695
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.45848169922828674
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.4784419536590576
INFO:root:Accuracy qa: 71.38791593695271
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.5219119787216187
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.34319543838500977
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.37647488713264465
INFO:root:Accuracy qa: 73.31436077057793
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.2634710371494293
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.26504769921302795
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.2954414486885071
INFO:root:Accuracy qa: 73.24868651488616
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.27804484963417053
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.24094364047050476
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.21098430454730988
INFO:root:Accuracy qa: 73.1830122591944
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.20843088626861572
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.1836434304714203
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.21257925033569336
INFO:root:Accuracy qa: 73.51138353765324
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.19781064987182617
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.1392422616481781
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.1969553381204605
INFO:root:Accuracy qa: 73.20490367775832
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.1630309671163559
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.15454712510108948
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.13925522565841675
INFO:root:Accuracy qa: 72.87653239929948
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.17339996993541718
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.13675129413604736
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.11925937980413437
INFO:root:Accuracy qa: 73.1830122591944
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 12
INFO:root:the best train acc ------ 73.51138353765324
INFO:root:********************************************************
Audio Counting num: 856
Audio Cmp num: 411
Visual Counting num: 937
Visual Loc num: 939
AV Ext num: 816
AV counting num: 937
AV Loc num: 576
AV Cmp num: 731
AV Temporal num: 531
INFO:root:Audio Counting Accuracy: 84.17 %
INFO:root:Audio Cmp Accuracy: 69.19 %
INFO:root:Audio Accuracy: 78.65 %
INFO:root:Visual Counting Accuracy: 78.28 %
INFO:root:Visual Loc Accuracy: 76.65 %
INFO:root:Visual Accuracy: 77.46 %
INFO:root:AV Ext Accuracy: 82.59 %
INFO:root:AV counting Accuracy: 74.07 %
INFO:root:AV Loc Accuracy: 62.61 %
INFO:root:AV Cmp Accuracy: 66.39 %
INFO:root:AV Temporal Accuracy: 64.60 %
INFO:root:AV Accuracy: 70.47 %
INFO:root:Overall Accuracy: 73.76 %
Audio Counting num: 856
Audio Cmp num: 411
Visual Counting num: 937
Visual Loc num: 939
AV Ext num: 816
AV counting num: 937
AV Loc num: 576
AV Cmp num: 731
AV Temporal num: 531
Audio Counting num: 856
Audio Cmp num: 411
Visual Counting num: 937
Visual Loc num: 939
AV Ext num: 816
AV counting num: 937
AV Loc num: 576
AV Cmp num: 731
AV Temporal num: 531
Audio Counting num: 856
Audio Cmp num: 411
Visual Counting num: 937
Visual Loc num: 939
AV Ext num: 816
AV counting num: 937
AV Loc num: 576
AV Cmp num: 731
AV Temporal num: 531
