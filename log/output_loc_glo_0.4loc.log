nohup: ignoring input
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
INFO:root:********************************************************
INFO:root:****************** Experiment setting ******************
INFO:root:label_train dir --------- ./data/json_update/avqa-train.json
INFO:root:label_val dir ----------- ./data/json_update/avqa-val.json
INFO:root:label_test dir ---------- ./data/json_update/avqa-test.json
INFO:root:audio dir --------------- ./data/feats/vggish
INFO:root:posi_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:nega_video dir ---------- /data/avst/r2plus1d_18/
INFO:root:batch size -------------- 32
INFO:root:num of epoches ---------- 15
INFO:root:learning rate ----------- 0.0001
INFO:root:name of model ----------- AVQA_Fusion_Net
INFO:root:train or test ----------- train
INFO:root:model save dir ---------- ./data/checkpoints/checkpoint_0.4loc
INFO:root:num of dataloader workers 2
INFO:root:gpu id ------------------ 0,1,2,3
INFO:root:********************************************************
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:root:------------ Audio-Visual Spatial-Temporal Model ------------ 

INFO:root:-------------- training dataset preparation --------------

INFO:root:--------- training dataset preparation completed ----------

INFO:root:length of training dataset ----------- 250

INFO:root:-------------- validation dataset preparation --------------

INFO:root:-------- validation dataset preparation completed ----------
INFO:root:length of validation dataset ----------- 143
INFO:root:-------------- start training --------------

/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
INFO:root:Train Epoch: 1 [0/31927 ]	Loss: 5.557408332824707
INFO:root:Train Epoch: 1 [3200/31927 ]	Loss: 1.5905249118804932
INFO:root:Train Epoch: 1 [6400/31927 ]	Loss: 1.38017737865448
INFO:root:Accuracy qa: 61.77758318739054
INFO:root:Train Epoch: 2 [0/31927 ]	Loss: 1.22958505153656
INFO:root:Train Epoch: 2 [3200/31927 ]	Loss: 1.3153759241104126
INFO:root:Train Epoch: 2 [6400/31927 ]	Loss: 1.271078109741211
INFO:root:Accuracy qa: 65.76182136602452
INFO:root:Train Epoch: 3 [0/31927 ]	Loss: 1.1121609210968018
INFO:root:Train Epoch: 3 [3200/31927 ]	Loss: 1.1066397428512573
INFO:root:Train Epoch: 3 [6400/31927 ]	Loss: 1.1820539236068726
INFO:root:Accuracy qa: 67.5569176882662
INFO:root:Train Epoch: 4 [0/31927 ]	Loss: 1.0101276636123657
INFO:root:Train Epoch: 4 [3200/31927 ]	Loss: 1.0304393768310547
INFO:root:Train Epoch: 4 [6400/31927 ]	Loss: 0.9316332340240479
INFO:root:Accuracy qa: 68.8922942206655
INFO:root:Train Epoch: 5 [0/31927 ]	Loss: 0.8720906972885132
INFO:root:Train Epoch: 5 [3200/31927 ]	Loss: 0.77659010887146
INFO:root:Train Epoch: 5 [6400/31927 ]	Loss: 0.835971474647522
INFO:root:Accuracy qa: 70.05253940455341
INFO:root:Train Epoch: 6 [0/31927 ]	Loss: 0.968828022480011
INFO:root:Train Epoch: 6 [3200/31927 ]	Loss: 0.834913969039917
INFO:root:Train Epoch: 6 [6400/31927 ]	Loss: 0.7526889443397522
INFO:root:Accuracy qa: 71.51926444833626
INFO:root:Train Epoch: 7 [0/31927 ]	Loss: 0.8328826427459717
INFO:root:Train Epoch: 7 [3200/31927 ]	Loss: 0.8032215237617493
INFO:root:Train Epoch: 7 [6400/31927 ]	Loss: 0.7486138343811035
INFO:root:Accuracy qa: 69.19877408056043
INFO:root:Train Epoch: 8 [0/31927 ]	Loss: 0.6295559406280518
INFO:root:Train Epoch: 8 [3200/31927 ]	Loss: 0.5806441307067871
INFO:root:Train Epoch: 8 [6400/31927 ]	Loss: 0.5780858397483826
INFO:root:Accuracy qa: 71.93520140105079
INFO:root:Train Epoch: 9 [0/31927 ]	Loss: 0.6346285343170166
INFO:root:Train Epoch: 9 [3200/31927 ]	Loss: 0.43135595321655273
INFO:root:Train Epoch: 9 [6400/31927 ]	Loss: 0.4702655076980591
INFO:root:Accuracy qa: 73.90542907180385
INFO:root:Train Epoch: 10 [0/31927 ]	Loss: 0.38294103741645813
INFO:root:Train Epoch: 10 [3200/31927 ]	Loss: 0.3830190896987915
INFO:root:Train Epoch: 10 [6400/31927 ]	Loss: 0.38472938537597656
INFO:root:Accuracy qa: 74.29947460595447
INFO:root:Train Epoch: 11 [0/31927 ]	Loss: 0.34780555963516235
INFO:root:Train Epoch: 11 [3200/31927 ]	Loss: 0.30143097043037415
INFO:root:Train Epoch: 11 [6400/31927 ]	Loss: 0.3179696202278137
INFO:root:Accuracy qa: 73.75218914185639
INFO:root:Train Epoch: 12 [0/31927 ]	Loss: 0.25919675827026367
INFO:root:Train Epoch: 12 [3200/31927 ]	Loss: 0.2935614585876465
INFO:root:Train Epoch: 12 [6400/31927 ]	Loss: 0.3000732660293579
INFO:root:Accuracy qa: 73.64273204903678
INFO:root:Train Epoch: 13 [0/31927 ]	Loss: 0.2619894742965698
INFO:root:Train Epoch: 13 [3200/31927 ]	Loss: 0.28615066409111023
INFO:root:Train Epoch: 13 [6400/31927 ]	Loss: 0.34866243600845337
INFO:root:Accuracy qa: 73.6646234676007
INFO:root:Train Epoch: 14 [0/31927 ]	Loss: 0.26060858368873596
INFO:root:Train Epoch: 14 [3200/31927 ]	Loss: 0.27121269702911377
INFO:root:Train Epoch: 14 [6400/31927 ]	Loss: 0.23502863943576813
INFO:root:Accuracy qa: 73.83975481611209
INFO:root:Train Epoch: 15 [0/31927 ]	Loss: 0.239947110414505
INFO:root:Train Epoch: 15 [3200/31927 ]	Loss: 0.23932909965515137
INFO:root:Train Epoch: 15 [6400/31927 ]	Loss: 0.2280416190624237
INFO:root:Accuracy qa: 73.92732049036778
INFO:root:-------------- end of training --------------

INFO:root:length of test dataset ----------- 9129
INFO:root:-------------- loading checkpoint ----------------
INFO:root:-------- checkpoint loading successfully ----------
INFO:root:********************************************************
INFO:root:the best epoch ---------- 10
INFO:root:the best train acc ------ 74.29947460595447
INFO:root:********************************************************
Audio Counting num: 859
Audio Cmp num: 410
Visual Counting num: 954
Visual Loc num: 956
AV Ext num: 799
AV counting num: 930
AV Loc num: 565
AV Cmp num: 730
AV Temporal num: 541
Audio Counting num: 859
Audio Cmp num: 410
Visual Counting num: 954
Visual Loc num: 956
AV Ext num: 799
AV counting num: 930
AV Loc num: 565
AV Cmp num: 730
AV Temporal num: 541
INFO:root:Audio Counting Accuracy: 84.46 %
INFO:root:Audio Cmp Accuracy: 69.02 %
INFO:root:Audio Accuracy: 78.77 %
INFO:root:Visual Counting Accuracy: 79.70 %
INFO:root:Visual Loc Accuracy: 78.04 %
INFO:root:Visual Accuracy: 78.86 %
INFO:root:AV Ext Accuracy: 80.87 %
INFO:root:AV counting Accuracy: 73.52 %
INFO:root:AV Loc Accuracy: 61.41 %
INFO:root:AV Cmp Accuracy: 66.30 %
INFO:root:AV Temporal Accuracy: 65.82 %
INFO:root:AV Accuracy: 69.96 %
INFO:root:Overall Accuracy: 73.87 %
Audio Counting num: 859
Audio Cmp num: 410
Visual Counting num: 954
Visual Loc num: 956
AV Ext num: 799
AV counting num: 930
AV Loc num: 565
AV Cmp num: 730
AV Temporal num: 541
Audio Counting num: 859
Audio Cmp num: 410
Visual Counting num: 954
Visual Loc num: 956
AV Ext num: 799
AV counting num: 930
AV Loc num: 565
AV Cmp num: 730
AV Temporal num: 541
