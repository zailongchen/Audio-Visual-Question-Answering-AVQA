{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842b4541-278f-4a4f-bb4d-db0960b00201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "from layers.attention_vis import Attention\n",
    "# from layers.attention import Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from models.visual_net import resnet18\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b822f95-f390-4570-9dde-be11a6baccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVQA_Fusion_Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AVQA_Fusion_Net, self).__init__()\n",
    "        self.device = 'cpu'\n",
    "        self.qst_vocab_size = 93\n",
    "        self.word_embed_size = 512\n",
    "        self.embed_dim_audio = 128\n",
    "        self.embed_dim_video = 512  # or 2048\n",
    "        self.hidden_dim = 256\n",
    "        self.num_classes = 42  # size of answer vocab\n",
    "        self.stage1_hops = 2\n",
    "        self.stage2_hops = 2\n",
    "        self.num_heads = 4\n",
    "        self.lstm_num_layers = 1\n",
    "        self.que_max_len = 10\n",
    "        # get the feature from [-2] layer of resnet18\n",
    "        # self.img_extractor = nn.Sequential(*list(resnet18(pretrained=True, modal=\"vision\").children())[:-1])\n",
    "        # img_extractor = models.video.r2plus1d_18(pretrained=True)\n",
    "        # self.img_extractor = nn.Sequential(*list(img_extractor.children())[:-1])\n",
    "        # for p in self.img_extractor.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        self.word2vec = nn.Embedding(self.qst_vocab_size, self.word_embed_size)\n",
    "        self.bi_lstm_question = DynamicLSTM(self.word_embed_size,self.hidden_dim,num_layers=self.lstm_num_layers,batch_first=True,bidirectional=True,)\n",
    "        self.bi_lstm_audio = DynamicLSTM(self.embed_dim_audio,self.hidden_dim,num_layers=self.lstm_num_layers,batch_first=True,bidirectional=True,)\n",
    "        self.bi_lstm_video = DynamicLSTM(self.embed_dim_video,self.hidden_dim,num_layers=self.lstm_num_layers,batch_first=True,bidirectional=True,)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.EAVF_fusion = nn.Linear(self.hidden_dim * 6, self.hidden_dim * 2)\n",
    "        self.MF_fusion = nn.Linear(self.hidden_dim * 2, self.hidden_dim * 2)\n",
    "        self.LAF_fusion = nn.Linear(self.hidden_dim * 2, self.hidden_dim * 2)\n",
    "        self.fc_ans = nn.Linear(self.hidden_dim * 2 * self.que_max_len, self.num_classes)\n",
    "\n",
    "    # (self, audio, visual_posi, visual_nega, question)\n",
    "    def forward(self, audio_posi, video_posi, video_nega, question):\n",
    "        '''\n",
    "        question      [B, C]\n",
    "        audio         [B, T, C]\n",
    "        video_posi    [B, T, C, H, W]\n",
    "        video_nega    [B, T, C, H, W]\n",
    "        '''\n",
    "        B, T, C = video_posi.size()\n",
    "        # question_memory_len = torch.sum(question != 0, dim=-1).to(self.device)\n",
    "        question_memory_len = torch.tensor([self.que_max_len for i in range(B)]).to(\n",
    "            self.device\n",
    "        )\n",
    "        # print(question_memory_len)\n",
    "        audio_memory_len = torch.tensor([T for i in range(B)]).to(self.device)\n",
    "        video_memory_len = torch.tensor([T for i in range(B)]).to(self.device)\n",
    "        # nonzeros_question = torch.tensor(question_memory_len).to(self.device)\n",
    "\n",
    "        question = self.word2vec(question)  # [B, maxseqlen, C] [B, 14, 512]\n",
    "\n",
    "        # question_memory [B, 14, 512], audio_memory [B, T, 512], video_*_memory [B, T, 512]\n",
    "        question_memory, (_, _) = self.bi_lstm_question(question, question_memory_len)\n",
    "        audio_memory, (_, _) = self.bi_lstm_audio(audio_posi, audio_memory_len)\n",
    "        video_posi_memory, (_, _) = self.bi_lstm_video(video_posi, video_memory_len)\n",
    "        video_nega_memory, (_, _) = self.bi_lstm_video(video_nega, video_memory_len)\n",
    "        # print('question_memory: ', question_memory.shape)\n",
    "        \n",
    "        # EAVF\n",
    "        av_feat = torch.cat((audio_memory, video_posi_memory),dim=-1,)\n",
    "        qav_feat = torch.cat((question_memory, av_feat),dim=-1,)\n",
    "        EAVF_feat = self.tanh(qav_feat)\n",
    "        EAVF_feat = self.EAVF_fusion(EAVF_feat)\n",
    "        EAVF_feat = self.tanh(EAVF_feat)\n",
    "        \n",
    "        # MF\n",
    "        qav_feat = audio_memory * video_posi_memory * question_memory\n",
    "        MF_feat = self.tanh(qav_feat)\n",
    "        MF_feat = self.MF_fusion(MF_feat)\n",
    "        MF_feat = self.tanh(MF_feat)\n",
    "        \n",
    "        # LAF\n",
    "        qav_feat = audio_memory * video_posi_memory * question_memory\n",
    "        LAF_feat = self.tanh(qav_feat)\n",
    "        LAF_feat = self.LAF_fusion(LAF_feat)\n",
    "        LAF_feat = self.tanh(LAF_feat)\n",
    "        \n",
    "        averaged_feature = (EAVF_feat + MF_feat + LAF_feat)/3\n",
    "        # print('averaged_feature: ', averaged_feature.shape)\n",
    "        \n",
    "        combined_feature = rearrange(averaged_feature, 'b t c -> b (t c)')\n",
    "        out = self.fc_ans(combined_feature)  # [batch_size, ans_vocab_size]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88330bae-1db9-4e9a-bec9-e4ea1bbd023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaged_feature:  torch.Size([2, 10, 512])\n",
      "\n",
      "out_qa feature dimension -----  torch.Size([2, 42])\n",
      "-- model constructing successfully --\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # args = parser.parse_args()\n",
    "    # args.device = 'cpu'\n",
    "    args = ''\n",
    "    model = AVQA_Fusion_Net(args)\n",
    "    model.eval()\n",
    "    audio = torch.randn(2, 10, 128)\n",
    "    video_posi = torch.randn(2, 10, 512)\n",
    "    video_nega = torch.randn(2, 10, 512)\n",
    "    question = np.array([np.random.randint(0, 93, 14), np.random.randint(0, 93, 14)])\n",
    "    question = torch.from_numpy(question).long()\n",
    "    out_qa = model(audio, video_posi, video_nega, question)\n",
    "    print('\\nout_qa feature dimension ----- ', out_qa.size())\n",
    "    # print('loc_et_posi_audio feature dimension ----- ', loc_et_posi_audio.size())\n",
    "    # print('loc_et_posi_video feature dimension ----- ', loc_et_posi_video.size())\n",
    "    # print('loc_et_nega_video feature dimension ----- ', loc_et_nega_video.size())\n",
    "    # print('glo_et_posi_audio feature dimension ----- ', glo_et_posi_audio.size())\n",
    "    # print('loc_et_nega_video feature dimension ----- ', glo_et_posi_video.size())\n",
    "    # print('glo_et_posi_video feature dimension ----- ', glo_et_nega_video.size())\n",
    "    # print('loc_att_map feature dimension ----- ', loc_att_map.size())\n",
    "    print('-- model constructing successfully --')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db7e9b-2301-4507-af2e-844b0f118bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64fd3b06-b9ea-4c88-8cf7-2715e6ff842f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# trihard loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9270b81-73c0-4284-8cb6-9182df4a099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "    Args:\n",
    "        margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "        # Compute pairwise distance, replace by the official when merged\n",
    "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "        print('\\ndist ----- 0', dist.shape)\n",
    "        print(dist)\n",
    "        dist = dist + dist.t()\n",
    "        print('\\ndist ----- 1', dist.shape)\n",
    "        print(dist)\n",
    "        # dist.addmm_(1, -2, inputs, inputs.t())\n",
    "        dist = torch.addmm(1, dist, -2, inputs, inputs.t())\n",
    "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
    "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e479c5a8-e71c-4af4-bceb-732d38c96483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dist ----- 0 torch.Size([32, 32])\n",
      "tensor([[519.9790, 519.9790, 519.9790,  ..., 519.9790, 519.9790, 519.9790],\n",
      "        [529.4755, 529.4755, 529.4755,  ..., 529.4755, 529.4755, 529.4755],\n",
      "        [503.3029, 503.3029, 503.3029,  ..., 503.3029, 503.3029, 503.3029],\n",
      "        ...,\n",
      "        [514.5906, 514.5906, 514.5906,  ..., 514.5906, 514.5906, 514.5906],\n",
      "        [560.9006, 560.9006, 560.9006,  ..., 560.9006, 560.9006, 560.9006],\n",
      "        [477.0782, 477.0782, 477.0782,  ..., 477.0782, 477.0782, 477.0782]])\n",
      "\n",
      "dist ----- 1 torch.Size([32, 32])\n",
      "tensor([[1039.9580, 1049.4546, 1023.2820,  ..., 1034.5696, 1080.8796,\n",
      "          997.0573],\n",
      "        [1049.4546, 1058.9510, 1032.7784,  ..., 1044.0662, 1090.3762,\n",
      "         1006.5538],\n",
      "        [1023.2820, 1032.7784, 1006.6059,  ..., 1017.8936, 1064.2036,\n",
      "          980.3812],\n",
      "        ...,\n",
      "        [1034.5696, 1044.0662, 1017.8936,  ..., 1029.1812, 1075.4912,\n",
      "          991.6688],\n",
      "        [1080.8796, 1090.3762, 1064.2036,  ..., 1075.4912, 1121.8013,\n",
      "         1037.9789],\n",
      "        [ 997.0573, 1006.5538,  980.3812,  ...,  991.6688, 1037.9789,\n",
      "          954.1565]])\n",
      "tensor(3.1462)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    target = [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8]\n",
    "    target = torch.tensor(target)\n",
    "    features = torch.randn(32, 512)\n",
    "    a = TripletLoss()\n",
    "    loss = a.forward(features, target)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03eee63-d6e3-4a26-b591-4d2c7f453d2e",
   "metadata": {},
   "source": [
    "# net test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dcf8294-0e2a-4f4f-b184-ae438ba459d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from models.visual_net_vis import resnet18\n",
    "\n",
    "\n",
    "class AVQA_AVatt_Grounding(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AVQA_AVatt_Grounding, self).__init__()\n",
    "\n",
    "        # for features\n",
    "        self.fc_a1 =  nn.Linear(128, 512)\n",
    "        self.fc_a2=nn.Linear(512,512)\n",
    "\n",
    "        # visual\n",
    "        self.visual_net = resnet18(pretrained=True)\n",
    "\n",
    "        # combine\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(128, 2)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc_gl=nn.Linear(1024,512)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, video_id, audio, visual):\n",
    "\n",
    "        ## audio features\n",
    "        audio_feat = F.relu(self.fc_a1(audio))\n",
    "        audio_feat=self.fc_a2(audio_feat)                      # [16, 20, 512]\n",
    "        (B, T, C) = audio_feat.size()\n",
    "        audio_feat = audio_feat.view(B*T, C)                # [320, 512]\n",
    "\n",
    "        ## visual, input: [16, 20, 3, 224, 224]\n",
    "        (B, T, C, H, W) = visual.size()\n",
    "        visual = visual.view(B * T, C, H, W)                # [320, 3, 224, 224]\n",
    "\n",
    "        v_feat_out_res18 = self.visual_net(visual)                    # [320, 512, 14, 14]\n",
    "        print('v_feat_out_res18 ----- ', v_feat_out_res18.size())\n",
    "        v_feat=self.avgpool(v_feat_out_res18)\n",
    "        visual_feat_before_grounding=v_feat.squeeze()     # 320 512\n",
    "        \n",
    "        (B, C, H, W) = v_feat_out_res18.size()\n",
    "        v_feat = v_feat_out_res18.view(B, C, H * W)\n",
    "        v_feat = v_feat.permute(0, 2, 1)  # B, HxW, C\n",
    "        visual = nn.functional.normalize(v_feat, dim=2)\n",
    "             \n",
    "        ## audio-visual grounding\n",
    "        audio_feat_aa = audio_feat.unsqueeze(-1)            # [320, 512, 1]\n",
    "        audio_feat_aa = nn.functional.normalize(audio_feat_aa, dim=1)\n",
    "        visual_feat = visual\n",
    "        print('visual_feat ----- ', visual_feat.size())\n",
    "        print('audio_feat_aa ----- ', audio_feat_aa.size())\n",
    "        x2_va = torch.matmul(visual_feat, audio_feat_aa).squeeze()\n",
    "\n",
    "        x2_p = F.softmax(x2_va, dim=-1).unsqueeze(-2)       # [320, 1, 196]\n",
    "        visual_feat_grd = torch.matmul(x2_p, visual_feat)\n",
    "        visual_feat_grd = visual_feat_grd.squeeze()         # [320, 512]   \n",
    "\n",
    "        visual_gl=torch.cat((visual_feat_before_grounding,visual_feat_grd),dim=-1)\n",
    "        visual_feat_grd=self.tanh(visual_gl)\n",
    "        visual_feat_grd=self.fc_gl(visual_feat_grd)\n",
    "\n",
    "        # combine a and v\n",
    "        feat = torch.cat((audio_feat, visual_feat_grd), dim=-1)     # [320, 1024]\n",
    "\n",
    "        feat = F.relu(self.fc1(feat))   # (1024, 512)\n",
    "        feat = F.relu(self.fc2(feat))   # (512, 256)\n",
    "        feat = F.relu(self.fc3(feat))   # (256, 128)\n",
    "        feat = self.fc4(feat)   # (128, 2)\n",
    "\n",
    "        return  x2_p, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15efa9ea-1318-43d9-86a4-60439a2367ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- >>> load pretrained res-18 <<< ----------\n",
      "\n",
      "v_feat_out_res18 -----  torch.Size([320, 512, 14, 14])\n",
      "visual_feat -----  torch.Size([320, 196, 512])\n",
      "audio_feat_aa -----  torch.Size([320, 512, 1])\n",
      "att_map -----  torch.Size([320, 1, 196])\n",
      "feat -----  torch.Size([320, 2])\n",
      "-- model constructing successfully --\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # args = parser.parse_args()\n",
    "    # args.device = 'cpu'\n",
    "    args = ''\n",
    "    model = AVQA_AVatt_Grounding()\n",
    "    model.eval()\n",
    "    audio = torch.randn(32, 10, 128)\n",
    "    video_posi = torch.randn(32, 10, 3, 224, 224)\n",
    "    question = np.array([np.random.randint(0, 93, 14), np.random.randint(0, 93, 14)])\n",
    "    att_map, feat = model(1, audio, video_posi)\n",
    "    print('att_map ----- ', att_map.size())\n",
    "    print('feat ----- ', feat.size())\n",
    "    print('-- model constructing successfully --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4c880-cbbc-4d64-87cd-dc4702eec8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czl",
   "language": "python",
   "name": "czl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
