{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842b4541-278f-4a4f-bb4d-db0960b00201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "from layers.attention import Attention\n",
    "# from layers.attention import Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "# from models.visual_net import resnet18\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b822f95-f390-4570-9dde-be11a6baccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''A two-feed-forward-layer module'''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.downsample = nn.Linear(d_in * 2, d_in)\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        x = torch.cat((x1, x2), dim=-1)\n",
    "        x = self.tanh(x)\n",
    "        x = F.relu(self.downsample(x))\n",
    "        x = self.layer_norm(x)\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "class self_att(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(self_att, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.self_att = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=0.1)\n",
    "        self.norm = nn.LayerNorm(self.hidden_dim * 2)\n",
    "    \n",
    "    def forward(self, q_k_v):\n",
    "        feat_att = self.self_att(q_k_v,q_k_v).squeeze(dim=1)\n",
    "        feat_att = q_k_v + feat_att\n",
    "        feat_att = self.norm(feat_att)\n",
    "        return feat_att\n",
    "\n",
    "# Co-attention between audio and video, question is treated as query\n",
    "class Global_Branch(nn.Module):\n",
    "    def __init__(self, hops, hidden_dim, num_heads, dropout=0.1):\n",
    "        super(Global_Branch, self).__init__()\n",
    "        self.hops = hops\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = 512\n",
    "        self.attention_audio = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.attention_video = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.attention_audio2video = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.attention_video2audio = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.FFN_audio = FeedForward(self.hidden_dim * 2, self.hidden_dim * 4)\n",
    "        self.FFN_video = FeedForward(self.hidden_dim * 2, self.hidden_dim * 4)\n",
    "        self.video_self_att = self_att(self.hidden_dim, num_heads)\n",
    "        self.audio_self_att = self_att(self.hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, audio_memory, video_memory):\n",
    "        audio_memory = self.audio_self_att(audio_memory)\n",
    "        video_memory = self.video_self_att(video_memory)\n",
    "        et_audio = audio_memory\n",
    "        et_video = video_memory\n",
    "        for _ in range(self.hops):\n",
    "            # audio branch\n",
    "            it_al_audio2audio = self.attention_audio(audio_memory, et_audio).squeeze(dim=1)\n",
    "            it_al_video2audio = self.attention_video2audio(audio_memory, et_video).squeeze(dim=1)\n",
    "            it_al_audio = (it_al_audio2audio + it_al_video2audio) / 2\n",
    "            # video branch\n",
    "            it_al_video2video = self.attention_video(video_memory, et_video).squeeze(dim=1)\n",
    "            it_al_audio2video = self.attention_audio2video(video_memory, et_audio).squeeze(dim=1)\n",
    "            it_al_video = (it_al_video2video + it_al_audio2video) / 2\n",
    "\n",
    "            # combined_feature = rearrange(combined_feature, 'b t c -> b (t c)')\n",
    "            et_audio = self.FFN_audio(it_al_audio, et_audio)\n",
    "            et_video = self.FFN_video(it_al_video, et_video)\n",
    "\n",
    "        return et_audio, et_video\n",
    "    \n",
    "# Co-attention between audio and video, question is treated as query\n",
    "class Question_Oriented_Attention(nn.Module):\n",
    "    def __init__(self, hops, hidden_dim, num_heads, dropout=0.1):\n",
    "        super(Question_Oriented_Attention, self).__init__()\n",
    "        self.hops = hops\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = 512\n",
    "        self.attention_audio = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.attention_video = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.attention_audio2video = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.attention_video2audio = Attention(self.hidden_dim * 2, n_head=num_heads, score_function='mlp', dropout=dropout)\n",
    "        self.FFN_audio = FeedForward(self.hidden_dim * 2, self.hidden_dim * 4)\n",
    "        self.FFN_video = FeedForward(self.hidden_dim * 2, self.hidden_dim * 4)\n",
    "        \n",
    "        self.video_self_att = self_att(self.hidden_dim, num_heads)\n",
    "        self.audio_self_att = self_att(self.hidden_dim, num_heads)\n",
    "        self.question_self_att = self_att(self.hidden_dim, num_heads)\n",
    "\n",
    "    def forward(self, question_memory, audio_memory, video_memory):\n",
    "        question_memory = self.question_self_att(question_memory)\n",
    "        audio_memory = self.audio_self_att(audio_memory)\n",
    "        video_memory = self.video_self_att(video_memory)\n",
    "        \n",
    "        et_audio = question_memory\n",
    "        et_video = question_memory\n",
    "        for _ in range(self.hops):\n",
    "            # audio branch\n",
    "            it_al_audio2audio = self.attention_audio(audio_memory, et_audio).squeeze(dim=1)\n",
    "            it_al_video2audio = self.attention_video2audio(audio_memory, et_video).squeeze(dim=1)\n",
    "            it_al_audio = (it_al_audio2audio + it_al_video2audio) / 2\n",
    "            # video branch\n",
    "            it_al_video2video = self.attention_video(video_memory, et_video).squeeze(dim=1)\n",
    "            it_al_audio2video = self.attention_audio2video(video_memory, et_audio).squeeze(dim=1)\n",
    "            it_al_video = (it_al_video2video + it_al_audio2video) / 2\n",
    "\n",
    "            # combined_feature = rearrange(combined_feature, 'b t c -> b (t c)')\n",
    "            et_audio = self.FFN_audio(it_al_audio, et_audio)\n",
    "            et_video = self.FFN_video(it_al_video, et_video)\n",
    "\n",
    "        return et_audio, et_video\n",
    "\n",
    "\n",
    "class AVQA_Fusion_Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AVQA_Fusion_Net, self).__init__()\n",
    "        self.device = 'cpu'\n",
    "        self.qst_vocab_size = 93\n",
    "        self.word_embed_size = 512\n",
    "        self.embed_dim_audio = 128\n",
    "        self.embed_dim_video = 512  # or 2048\n",
    "        self.hidden_dim = 256\n",
    "        self.num_classes = 42  # size of answer vocab\n",
    "        self.stage1_hops = 2\n",
    "        self.stage2_hops = 2\n",
    "        self.num_heads = 4\n",
    "        self.lstm_num_layers = 1\n",
    "        self.que_max_len = 14\n",
    "        # get the feature from [-2] layer of resnet18\n",
    "        # self.img_extractor = nn.Sequential(*list(resnet18(pretrained=True, modal=\"vision\").children())[:-1])\n",
    "        img_extractor = models.video.r2plus1d_18(pretrained=True)\n",
    "        self.img_extractor = nn.Sequential(*list(img_extractor.children())[:-1])\n",
    "        # for p in self.img_extractor.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        self.word2vec = nn.Embedding(self.qst_vocab_size, self.word_embed_size)\n",
    "        self.bi_lstm_question = DynamicLSTM(self.word_embed_size,self.hidden_dim,num_layers=self.lstm_num_layers,batch_first=True,bidirectional=True,)\n",
    "        self.bi_lstm_audio = DynamicLSTM(self.embed_dim_audio,self.hidden_dim,num_layers=self.lstm_num_layers,batch_first=True,bidirectional=True,)\n",
    "        self.bi_lstm_video = DynamicLSTM(self.embed_dim_video,self.hidden_dim,num_layers=self.lstm_num_layers,batch_first=True,bidirectional=True,)\n",
    "\n",
    "        self.Local_Branch = Question_Oriented_Attention(self.stage1_hops, self.hidden_dim, self.num_heads)\n",
    "        self.Global_Branch = Global_Branch(self.stage1_hops, self.hidden_dim, self.num_heads)\n",
    "        \n",
    "        self.Global_Local_Fusion = Question_Oriented_Attention(self.stage2_hops, self.hidden_dim, self.num_heads)\n",
    "        \n",
    "        self.local_fusion = FeedForward(self.hidden_dim * 2, self.hidden_dim * 4)\n",
    "        self.global_fusion = FeedForward(self.hidden_dim * 2, self.hidden_dim * 4)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_fusion = nn.Linear(self.hidden_dim * 4, self.hidden_dim * 2)\n",
    "        self.fc_ans = nn.Linear(self.hidden_dim * 2 * self.que_max_len, self.num_classes)\n",
    "\n",
    "    # (self, audio, visual_posi, visual_nega, question)\n",
    "    def forward(self, audio_posi, video_posi, video_nega, question):\n",
    "        '''\n",
    "        question      [B, C]\n",
    "        audio         [B, T, C]\n",
    "        video_posi    [B, T, C, H, W]\n",
    "        video_nega    [B, T, C, H, W]\n",
    "        '''\n",
    "        B, T, C,_,_,_ = video_posi.size()\n",
    "        # question_memory_len = torch.sum(question != 0, dim=-1).to(self.device)\n",
    "        question_memory_len = torch.tensor([self.que_max_len for i in range(B)]).to(\n",
    "            self.device\n",
    "        )\n",
    "        # print(question_memory_len)\n",
    "        audio_memory_len = torch.tensor([T for i in range(B)]).to(self.device)\n",
    "        video_memory_len = torch.tensor([T for i in range(B)]).to(self.device)\n",
    "        # nonzeros_question = torch.tensor(question_memory_len).to(self.device)\n",
    "\n",
    "        question = self.word2vec(question)  # [B, maxseqlen, C] [B, 14, 512]\n",
    "        video_posi = rearrange(video_posi, 'b t c k h w -> (b t) c k h w')\n",
    "        video_posi = self.img_extractor(video_posi)  # [B*T, C, h w] [B*T, 512, 1, 1]\n",
    "        video_posi = rearrange(video_posi, '(b t) c k h w -> b t (c k h w)', t=T)\n",
    "\n",
    "        video_nega = rearrange(video_nega, 'b t c k h w -> (b t) c k h w')\n",
    "        video_nega = self.img_extractor(video_nega)  # [B*T, C, h w] [B*T, 512, 1, 1]\n",
    "        video_nega = rearrange(video_nega, '(b t) c k h w -> b t (c k h w)', t=T)\n",
    "\n",
    "        # question_memory [B, 14, 512], audio_memory [B, T, 512], video_*_memory [B, T, 512]\n",
    "        question_memory, (_, _) = self.bi_lstm_question(question, question_memory_len)\n",
    "        audio_memory, (_, _) = self.bi_lstm_audio(audio_posi, audio_memory_len)\n",
    "        video_posi_memory, (_, _) = self.bi_lstm_video(video_posi, video_memory_len)\n",
    "        video_nega_memory, (_, _) = self.bi_lstm_video(video_nega, video_memory_len)\n",
    "        # print('question_memory: ', question_memory.shape)\n",
    "        \n",
    "        # local video feature\n",
    "        # stage_1 co-attention branch of positive audio and positive video with question query\n",
    "        loc_et_posi_audio, loc_et_posi_video = self.Local_Branch(question_memory, audio_memory, video_posi_memory)\n",
    "        # stage_1 co-attention branch of positive audio and negative video\n",
    "        _, loc_et_nega_video = self.Local_Branch(question_memory, audio_memory, video_nega_memory)\n",
    "        \n",
    "        # global video feature\n",
    "        # stage_1 co-attention branch of positive audio and positive video\n",
    "        glo_et_posi_audio, glo_et_posi_video = self.Global_Branch(audio_memory, video_posi_memory)\n",
    "        _, glo_et_nega_video = self.Global_Branch(audio_memory, video_nega_memory)\n",
    "        \n",
    "        local_feature = self.local_fusion(loc_et_posi_audio, loc_et_posi_video)\n",
    "        global_feature = self.global_fusion(glo_et_posi_audio, glo_et_posi_video)\n",
    "        \n",
    "        # stage_2 co-attention branch of stage_1's output\n",
    "        audio_feat_att, visual_posi_feat_att = self.Global_Local_Fusion(question_memory, local_feature, global_feature)\n",
    "\n",
    "        # fusion between fused video and fused audio\n",
    "        feat = torch.cat((audio_feat_att, visual_posi_feat_att),dim=-1,)\n",
    "        feat = self.tanh(feat)\n",
    "        feat = self.fc_fusion(feat)\n",
    "        combined_feature = self.tanh(feat)\n",
    "        \n",
    "        combined_feature = rearrange(combined_feature, 'b t c -> b (t c)')\n",
    "        out = self.fc_ans(combined_feature)  # [batch_size, ans_vocab_size]\n",
    "\n",
    "        return (\n",
    "            out,\n",
    "            rearrange(loc_et_posi_audio, 'b t c -> b (t c)'),\n",
    "            rearrange(loc_et_posi_video, 'b t c -> b (t c)'),\n",
    "            rearrange(loc_et_nega_video, 'b t c -> b (t c)'),\n",
    "            rearrange(glo_et_posi_audio, 'b t c -> b (t c)'),\n",
    "            rearrange(glo_et_posi_video, 'b t c -> b (t c)'),\n",
    "            rearrange(glo_et_nega_video, 'b t c -> b (t c)'),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88330bae-1db9-4e9a-bec9-e4ea1bbd023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=R2Plus1D_18_Weights.KINETICS400_V1`. You can also use `weights=R2Plus1D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/jovyan/conda-envs/czl/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "out_qa feature dimension -----  torch.Size([2, 42])\n",
      "loc_et_posi_audio feature dimension -----  torch.Size([2, 7168])\n",
      "loc_et_posi_video feature dimension -----  torch.Size([2, 7168])\n",
      "loc_et_nega_video feature dimension -----  torch.Size([2, 7168])\n",
      "glo_et_posi_audio feature dimension -----  torch.Size([2, 5120])\n",
      "loc_et_nega_video feature dimension -----  torch.Size([2, 5120])\n",
      "glo_et_posi_video feature dimension -----  torch.Size([2, 5120])\n",
      "-- model constructing successfully --\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # args = parser.parse_args()\n",
    "    # args.device = 'cpu'\n",
    "    args = ''\n",
    "    model = AVQA_Fusion_Net(args)\n",
    "    model.eval()\n",
    "    audio = torch.randn(2, 10, 128)\n",
    "    video_posi = torch.randn(2, 10, 3, 1, 112, 112)\n",
    "    video_nega = torch.randn(2, 10, 3, 1, 112, 112)\n",
    "    question = np.array([np.random.randint(0, 93, 14), np.random.randint(0, 93, 14)])\n",
    "    question = torch.from_numpy(question).long()\n",
    "    (out_qa, loc_et_posi_audio, loc_et_posi_video, loc_et_nega_video,\n",
    "    glo_et_posi_audio, glo_et_posi_video, glo_et_nega_video) = model(audio, video_posi, video_nega, question)\n",
    "    print('\\nout_qa feature dimension ----- ', out_qa.size())\n",
    "    print('loc_et_posi_audio feature dimension ----- ', loc_et_posi_audio.size())\n",
    "    print('loc_et_posi_video feature dimension ----- ', loc_et_posi_video.size())\n",
    "    print('loc_et_nega_video feature dimension ----- ', loc_et_nega_video.size())\n",
    "    print('glo_et_posi_audio feature dimension ----- ', glo_et_posi_audio.size())\n",
    "    print('loc_et_nega_video feature dimension ----- ', glo_et_posi_video.size())\n",
    "    print('glo_et_posi_video feature dimension ----- ', glo_et_nega_video.size())\n",
    "    print('-- model constructing successfully --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11db7e9b-2301-4507-af2e-844b0f118bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64fd3b06-b9ea-4c88-8cf7-2715e6ff842f",
   "metadata": {},
   "source": [
    "# trihard loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9270b81-73c0-4284-8cb6-9182df4a099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "    Args:\n",
    "        margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "        # Compute pairwise distance, replace by the official when merged\n",
    "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "        print('\\ndist ----- 0', dist.shape)\n",
    "        print(dist)\n",
    "        dist = dist + dist.t()\n",
    "        print('\\ndist ----- 1', dist.shape)\n",
    "        print(dist)\n",
    "        # dist.addmm_(1, -2, inputs, inputs.t())\n",
    "        dist = torch.addmm(1, dist, -2, inputs, inputs.t())\n",
    "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
    "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e479c5a8-e71c-4af4-bceb-732d38c96483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dist ----- 0 torch.Size([32, 32])\n",
      "tensor([[519.9790, 519.9790, 519.9790,  ..., 519.9790, 519.9790, 519.9790],\n",
      "        [529.4755, 529.4755, 529.4755,  ..., 529.4755, 529.4755, 529.4755],\n",
      "        [503.3029, 503.3029, 503.3029,  ..., 503.3029, 503.3029, 503.3029],\n",
      "        ...,\n",
      "        [514.5906, 514.5906, 514.5906,  ..., 514.5906, 514.5906, 514.5906],\n",
      "        [560.9006, 560.9006, 560.9006,  ..., 560.9006, 560.9006, 560.9006],\n",
      "        [477.0782, 477.0782, 477.0782,  ..., 477.0782, 477.0782, 477.0782]])\n",
      "\n",
      "dist ----- 1 torch.Size([32, 32])\n",
      "tensor([[1039.9580, 1049.4546, 1023.2820,  ..., 1034.5696, 1080.8796,\n",
      "          997.0573],\n",
      "        [1049.4546, 1058.9510, 1032.7784,  ..., 1044.0662, 1090.3762,\n",
      "         1006.5538],\n",
      "        [1023.2820, 1032.7784, 1006.6059,  ..., 1017.8936, 1064.2036,\n",
      "          980.3812],\n",
      "        ...,\n",
      "        [1034.5696, 1044.0662, 1017.8936,  ..., 1029.1812, 1075.4912,\n",
      "          991.6688],\n",
      "        [1080.8796, 1090.3762, 1064.2036,  ..., 1075.4912, 1121.8013,\n",
      "         1037.9789],\n",
      "        [ 997.0573, 1006.5538,  980.3812,  ...,  991.6688, 1037.9789,\n",
      "          954.1565]])\n",
      "tensor(3.1462)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    target = [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8]\n",
    "    target = torch.tensor(target)\n",
    "    features = torch.randn(32, 512)\n",
    "    a = TripletLoss()\n",
    "    loss = a.forward(features, target)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf8294-0e2a-4f4f-b184-ae438ba459d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czl",
   "language": "python",
   "name": "czl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
